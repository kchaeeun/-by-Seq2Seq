{"cells":[{"cell_type":"markdown","source":["#### ë§ì¶¤ë²•ì˜ í† í° ì²˜ë¦¬ë§Œ ì´ê³³ì—ì„œ í•˜ê³  í•™ìŠµì€ ë¬¸ë²• ì „í™˜ ì½”ë“œ íŒŒì¼ì—ì„œ í–ˆìŠµë‹ˆë‹¤"],"metadata":{"id":"8UFMegY1vR9G"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"tA5mD5aGlBH3","executionInfo":{"status":"ok","timestamp":1702921949541,"user_tz":-540,"elapsed":25486,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["import pandas as pd\n","import urllib.request\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","from google.colab import drive\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","source":["# Google Drive ë§ˆìš´íŠ¸\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMnjmxah_BdQ","outputId":"ec3a2b05-30c5-4128-ff33-d6f4fbdc15a0","executionInfo":{"status":"ok","timestamp":1702921953690,"user_tz":-540,"elapsed":4168,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YSdjdCs0lMMJ","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":2267,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["# ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ì„¤ì •\n","data_dir = '/content/gdrive/MyDrive/Colab Notebooks/'\n","\n","# íŒŒì¼ëª… ì§€ì •\n","file_name = 'combined.csv'\n","\n","file_name_grammer = 'spell_dataset7.csv'\n","\n","# ì „ì²´ íŒŒì¼ ê²½ë¡œ ìƒì„±\n","file_path = os.path.join(data_dir, file_name)\n","file_path_grammer = os.path.join(data_dir, file_name_grammer)\n","\n","# CSV íŒŒì¼ ì½ê¸°\n","df = pd.read_csv(file_path)\n","df_gr = pd.read_csv(file_path_grammer)"]},{"cell_type":"code","source":["print(file_path_grammer)"],"metadata":{"id":"sZ0U2OXCtx4z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b93ae552-6b59-4364-d3e1-1cbbdee953a1","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":4,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/spell_dataset7.csv\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HxuiYiB_yHkj","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":2,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["from tensorflow.keras.optimizers.schedules import LearningRateSchedule"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fHMzeiROlPJp","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":2,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["# ìµœì¢… ë²„ì „\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, position, d_model, name=\"positional_encoding\"):\n","        super(PositionalEncoding, self).__init__(name=name)\n","        self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","    def get_angles(self, position, i, d_model):\n","        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","        return position * angles\n","\n","    def positional_encoding(self, position, d_model):\n","        angle_rads = self.get_angles(\n","            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","            d_model=d_model\n","        )\n","\n","        sines = tf.math.sin(angle_rads[:, 0::2])\n","        cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","        pos_encoding = tf.concat([sines, cosines], axis=-1)\n","        pos_encoding = pos_encoding[tf.newaxis, ...]\n","\n","        return tf.cast(pos_encoding, tf.float32)\n","\n","    def call(self, inputs):\n","        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","    def get_config(self):\n","        return {}\n","\n","\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # query í¬ê¸° : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","  # key í¬ê¸° : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","  # value í¬ê¸° : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","  # padding_mask : (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´)\n","\n","  # Qì™€ Kì˜ ê³±. ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬.\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # ìŠ¤ì¼€ì¼ë§\n","  # dkì˜ ë£¨íŠ¸ê°’ìœ¼ë¡œ ë‚˜ëˆ ì¤€ë‹¤.\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # ë§ˆìŠ¤í‚¹. ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì˜ ë§ˆìŠ¤í‚¹ í•  ìœ„ì¹˜ì— ë§¤ìš° ì‘ì€ ìŒìˆ˜ê°’ì„ ë„£ëŠ”ë‹¤.\n","  # ë§¤ìš° ì‘ì€ ê°’ì´ë¯€ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì§€ë‚˜ë©´ í–‰ë ¬ì˜ í•´ë‹¹ ìœ„ì¹˜ì˜ ê°’ì€ 0ì´ ëœë‹¤.\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ë§ˆì§€ë§‰ ì°¨ì›ì¸ keyì˜ ë¬¸ì¥ ê¸¸ì´ ë°©í–¥ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.\n","  # attention weight : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, keyì˜ ë¬¸ì¥ ê¸¸ì´)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # output : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    # d_modelì„ num_headsë¡œ ë‚˜ëˆˆ ê°’.\n","    # ë…¼ë¬¸ ê¸°ì¤€ : 64\n","    self.depth = d_model // self.num_heads\n","\n","    # WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    # WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  # num_heads ê°œìˆ˜ë§Œí¼ q, k, vë¥¼ splití•˜ëŠ” í•¨ìˆ˜\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # 1. WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì§€ë‚˜ê¸°\n","    # q : (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n","    # k : (batch_size, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n","    # v : (batch_size, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n","    # ì°¸ê³ ) ì¸ì½”ë”(k, v)-ë””ì½”ë”(q) ì–´í…ì…˜ì—ì„œëŠ” query ê¸¸ì´ì™€ key, valueì˜ ê¸¸ì´ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 2. í—¤ë“œ ë‚˜ëˆ„ê¸°\n","    # q : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","    # k : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","    # v : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 3. ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜. ì•ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ ì‚¬ìš©.\n","    # (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n","    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n","    # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, num_heads, d_model/num_heads)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 4. í—¤ë“œ ì—°ê²°(concatenate)í•˜ê¸°\n","    # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 5. WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì§€ë‚˜ê¸°\n","    # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs\n","\n","def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´)\n","  return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","  # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ì²«ë²ˆì§¸ ì„œë¸Œì¸µ / ì…€í”„ ì–´í…ì…˜)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': padding_mask # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n","      })\n","\n","  # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ (ë‘ë²ˆì§¸ ì„œë¸Œì¸µ)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","def encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # í¬ì§€ì…”ë„ ì¸ì½”ë”© + ë“œë¡­ì•„ì›ƒ\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # ì¸ì½”ë”ë¥¼ num_layersê°œ ìŒ“ê¸°\n","  for i in range(num_layers):\n","    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","# ë””ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µ(sublayer)ì—ì„œ ë¯¸ë˜ í† í°ì„ Maskí•˜ëŠ” í•¨ìˆ˜\n","def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x) # íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨\n","  return tf.maximum(look_ahead_mask, padding_mask)\n","\n","def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","\n","  # ë””ì½”ë”ëŠ” ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ)ì™€ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) ë‘˜ ë‹¤ ì‚¬ìš©.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ì²«ë²ˆì§¸ ì„œë¸Œì¸µ / ë§ˆìŠ¤í¬ë“œ ì…€í”„ ì–´í…ì…˜)\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': look_ahead_mask # ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬\n","      })\n","\n","  # ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ë‘ë²ˆì§¸ ì„œë¸Œì¸µ / ë””ì½”ë”-ì¸ì½”ë” ì–´í…ì…˜)\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n","          'mask': padding_mask # íŒ¨ë”© ë§ˆìŠ¤í¬\n","      })\n","\n","  # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ (ì„¸ë²ˆì§¸ ì„œë¸Œì¸µ)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™”\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def decoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","\n","  # ë””ì½”ë”ëŠ” ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ)ì™€ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) ë‘˜ ë‹¤ ì‚¬ìš©.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # í¬ì§€ì…”ë„ ì¸ì½”ë”© + ë“œë¡­ì•„ì›ƒ\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # ë””ì½”ë”ë¥¼ num_layersê°œ ìŒ“ê¸°\n","  for i in range(num_layers):\n","    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def transformer(vocab_size, num_layers, dff,\n","                d_model, num_heads, dropout,\n","                name=\"transformer\"):\n","\n","  # ì¸ì½”ë”ì˜ ì…ë ¥\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # ë””ì½”ë”ì˜ ì…ë ¥\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  # ì¸ì½”ë”ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","\n","  # ë””ì½”ë”ì˜ ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ)\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask, output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","\n","  # ë””ì½”ë”ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ)\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  # ì¸ì½”ë”ì˜ ì¶œë ¥ì€ enc_outputs. ë””ì½”ë”ë¡œ ì „ë‹¬ëœë‹¤.\n","  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask]) # ì¸ì½”ë”ì˜ ì…ë ¥ì€ ì…ë ¥ ë¬¸ì¥ê³¼ íŒ¨ë”© ë§ˆìŠ¤í¬\n","\n","  # ë””ì½”ë”ì˜ ì¶œë ¥ì€ dec_outputs. ì¶œë ¥ì¸µìœ¼ë¡œ ì „ë‹¬ëœë‹¤.\n","  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•œ ì¶œë ¥ì¸µ\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n","\n","def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, dtype=tf.float32)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","    def get_config(self):\n","        return {\n","            'd_model': self.d_model.numpy(),\n","            'warmup_steps': self.warmup_steps,\n","        }\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"YTKKZsGLosDm","executionInfo":{"status":"ok","timestamp":1702921957015,"user_tz":-540,"elapsed":1062,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["def preprocess_sentence(sentence):\n","  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","  sentence = sentence.strip()\n","  return sentence"]},{"cell_type":"code","source":["# NULL ê°’ì´ ìˆëŠ” í–‰ì„ ì œê±°\n","df = df.dropna()\n","df_gr = df_gr.dropna()\n","\n","# ì œê±°ëœ í›„ì˜ ë°ì´í„° í™•ì¸\n","print(df.isnull().sum())\n","print(df_gr.isnull().sum())"],"metadata":{"id":"7R3iIZ_nbxu1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7403f5c7-cc0a-4a84-9295-681dff542603","executionInfo":{"status":"ok","timestamp":1702921957015,"user_tz":-540,"elapsed":3,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Q    0\n","A    0\n","dtype: int64\n","Q    0\n","A    0\n","dtype: int64\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KNMY75I6lf6S","executionInfo":{"status":"ok","timestamp":1702921967062,"user_tz":-540,"elapsed":10049,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["questions = []\n","for sentence in df['Q']:\n","    # êµ¬ë‘ì ì— ëŒ€í•´ì„œ ë„ì–´ì“°ê¸°\n","    # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    questions.append(sentence)\n","\n","answers = []\n","for sentence in df['A']:\n","    # êµ¬ë‘ì ì— ëŒ€í•´ì„œ ë„ì–´ì“°ê¸°\n","    # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    answers.append(sentence)\n","\n","questions_gr = []\n","for sentence in df_gr['Q']:\n","    # êµ¬ë‘ì ì— ëŒ€í•´ì„œ ë„ì–´ì“°ê¸°\n","    # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    questions_gr.append(sentence)\n","\n","answers_gr = []\n","for sentence in df_gr['A']:\n","    # êµ¬ë‘ì ì— ëŒ€í•´ì„œ ë„ì–´ì“°ê¸°\n","    # ex) 12ì‹œ ë•¡! -> 12ì‹œ ë•¡ !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    answers_gr.append(sentence)\n","\n","\n","# Assuming df is the DataFrame containing your data\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Drop rows with NaN values\n","train_df = train_df.dropna()\n","test_df = test_df.dropna()\n","\n","\n","train_questions = train_df['Q']\n","train_answers = train_df['A']\n","\n","test_questions = test_df['Q']\n","test_answers = test_df['A']\n","\n","train_df_gr, test_df_gr = train_test_split(df_gr, test_size=0.2, random_state=42)\n","\n","train_df_gr = train_df_gr.dropna()\n","test_df_gr = test_df_gr.dropna()\n","\n","train_questions_gr = train_df_gr['Q']\n","train_answers_gr = train_df_gr['A']\n","\n","test_questions_gr = test_df_gr['Q']\n","test_answers_gr = test_df_gr['A']"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PdBQBr2Clh7i","executionInfo":{"status":"ok","timestamp":1702922186611,"user_tz":-540,"elapsed":217398,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ëª¨ë‘ í¬í•¨í•œ ë‹¨ì–´ ì§‘í•©(Vocabulary) ìƒì„±\n","tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**13)\n","\n","# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ëŒ€í•œ ì •ìˆ˜ ë¶€ì—¬.\n","\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","VOCAB_SIZE = tokenizer.vocab_size + 2\n","MAX_LENGTH = 10"]},{"cell_type":"code","source":["# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ëª¨ë‘ í¬í•¨í•œ ë‹¨ì–´ ì§‘í•©(Vocabulary) ìƒì„±\n","tokenizer_gr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions_gr + answers_gr, target_vocab_size=2**13)\n","\n","# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ëŒ€í•œ ì •ìˆ˜ ë¶€ì—¬.\n","\n","START_TOKEN_gr, END_TOKEN_gr = [tokenizer_gr.vocab_size], [tokenizer_gr.vocab_size + 1]\n","VOCAB_SIZE_gr = tokenizer_gr.vocab_size + 2"],"metadata":{"id":"ZUJ1DUttjAG3","executionInfo":{"status":"ok","timestamp":1702922393606,"user_tz":-540,"elapsed":207093,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # encode(í† í°í™” + ì •ìˆ˜ ì¸ì½”ë”©), ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","\n","    tokenized_inputs.append(sentence1)\n","    tokenized_outputs.append(sentence2)\n","\n","  # íŒ¨ë”©\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs\n","\n","questions, answers = tokenize_and_filter(questions, answers)\n","\n","train_questions, train_answers = tokenize_and_filter(train_questions, train_answers)\n","test_questions, test_answers = tokenize_and_filter(test_questions, test_answers)\n","\n","def tokenize_and_filter_gr(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # encode(í† í°í™” + ì •ìˆ˜ ì¸ì½”ë”©), ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€\n","    sentence1 = START_TOKEN_gr + tokenizer.encode(sentence1) + END_TOKEN_gr\n","    sentence2 = START_TOKEN_gr + tokenizer.encode(sentence2) + END_TOKEN_gr\n","\n","    tokenized_inputs.append(sentence1)\n","    tokenized_outputs.append(sentence2)\n","\n","  # íŒ¨ë”©\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=20, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=20, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs\n","\n","questions_gr, answers_gr = tokenize_and_filter_gr(questions_gr, answers_gr)\n","\n","train_questions_gr, train_answers_gr = tokenize_and_filter_gr(train_questions_gr, train_answers_gr)\n","test_questions_gr, test_answers_gr = tokenize_and_filter_gr(test_questions_gr, test_answers_gr)"],"metadata":{"id":"c8X0F6vHZzYk","executionInfo":{"status":"ok","timestamp":1702922430370,"user_tz":-540,"elapsed":36835,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"jtMhk_LelxG6","executionInfo":{"status":"ok","timestamp":1702922430398,"user_tz":-540,"elapsed":943,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["# í…ì„œí”Œë¡œìš° datasetì„ ì´ìš©í•˜ì—¬ ì…”í”Œ(shuffle)ì„ ìˆ˜í–‰í•˜ë˜, ë°°ì¹˜ í¬ê¸°ë¡œ ë°ì´í„°ë¥¼ ë¬¶ëŠ”ë‹¤.\n","# ë˜í•œ ì´ ê³¼ì •ì—ì„œ êµì‚¬ ê°•ìš”(teacher forcing)ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ë””ì½”ë”ì˜ ì…ë ¥ê³¼ ì‹¤ì œê°’ ì‹œí€€ìŠ¤ë¥¼ êµ¬ì„±í•œë‹¤.\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","# í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': train_questions,\n","        'dec_inputs': train_answers[:, :-1]\n","    },\n","    {\n","        'outputs': train_answers[:, 1:]\n","    },\n","))\n","\n","# ì…”í”Œ, ë°°ì¹˜ ë° êµì‚¬ ê°•ìš” ì„¤ì •\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n","test_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': test_questions,\n","        'dec_inputs': test_answers[:, :-1]\n","    },\n","    {\n","        'outputs': test_answers[:, 1:]\n","    },\n","))\n","\n","# ë°°ì¹˜ ì„¤ì •\n","test_dataset = test_dataset.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"0bLKqU_kmBeq","executionInfo":{"status":"ok","timestamp":1702920411655,"user_tz":-540,"elapsed":7899,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["# Hyper-parameters\n","VOCAB_SIZE = 10000\n","NUM_LAYERS = 4\n","D_MODEL = 256\n","NUM_HEADS = 8\n","DFF = 512\n","DROPOUT = 0.1\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    dff=DFF,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"oN-hVP2ZmGdY","executionInfo":{"status":"ok","timestamp":1702920411656,"user_tz":-540,"elapsed":49,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["MAX_LENGTH = 10\n","\n","\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"]},{"cell_type":"code","source":["EPOCHS = 10\n","\n","# ëª¨ë¸ í•™ìŠµ\n","model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset)\n","\n","\n","# ì •í™•ë„ ì‹œê°í™”\n","import matplotlib.pyplot as plt\n","\n","# Assuming that 'accuracy' is a metric in your model\n","plt.plot(model.history.history['accuracy'], label='Training Accuracy')\n","plt.plot(model.history.history['val_accuracy'], label='Testing Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"Zuh6M6UbhnQI","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"error","timestamp":1702921685100,"user_tz":-540,"elapsed":1273490,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}},"outputId":"deb3d786-94c9-4b34-e317-86645b825a39"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1240/1724 [====================>.........] - ETA: 8:04 - loss: 5.3297 - accuracy: 0.2695"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-365273ba3799>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ëª¨ë¸ í•™ìŠµ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyGfCKSD2DOA","executionInfo":{"status":"aborted","timestamp":1702921685101,"user_tz":-540,"elapsed":6,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["model.save('/content/gdrive/My Drive/Colab Notebooks/model_formal', save_format='tf')  # Save as TensorFlow SavedModel\n","# or"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3cso6Bp13J6l","executionInfo":{"status":"ok","timestamp":1702922456609,"user_tz":-540,"elapsed":27147,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"outputs":[],"source":["# informal -> formal ë³€í™˜ í•™ìŠµ ëª¨ë¸\n","with tf.keras.utils.custom_object_scope({'CustomSchedule': CustomSchedule, 'loss_function': loss_function}):\n","    loaded_model_1 = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_formal', custom_objects={'MultiHeadAttention': MultiHeadAttention})"]},{"cell_type":"code","source":["# ë§ì¶¤ë²• ì²˜ë¦¬ í•™ìŠµ ëª¨ë¸\n","with tf.keras.utils.custom_object_scope({'CustomSchedule': CustomSchedule, 'loss_function': loss_function}):\n","    loaded_model_2 = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_grammer', custom_objects={'MultiHeadAttention': MultiHeadAttention})"],"metadata":{"id":"wT0XDwaF8FSF","executionInfo":{"status":"ok","timestamp":1702922481917,"user_tz":-540,"elapsed":25375,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(test_dataset)\n","print(f'Test Accuracy: {test_accuracy}')"],"metadata":{"id":"gTrIPsx1rN7u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7621460d-2fc7-42a7-f28b-913e39ac7b18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["432/432 [==============================] - 151s 342ms/step - loss: 4.0398 - accuracy: 0.3733\n"]}]},{"cell_type":"code","source":["def evaluate_1(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN, 0)\n","\n","  # ë””ì½”ë”ì˜ ì˜ˆì¸¡ ì‹œì‘\n","  for i in range(MAX_LENGTH):\n","    predictions = loaded_model_1(inputs=[sentence, output], training=False)\n","\n","    # í˜„ì¬(ë§ˆì§€ë§‰) ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë°›ì•„ì˜¨ë‹¤.\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # ë§Œì•½ ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    # ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ì¶œë ¥ì— ì—°ê²°í•œë‹¤.\n","    # ì´ëŠ” forë¬¸ì„ í†µí•´ì„œ ë””ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ì˜ˆì •ì´ë‹¤.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n","\n","\n","def predict_formal(sentence):\n","  prediction = evaluate_1(sentence)\n","\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i < tokenizer.vocab_size])\n","\n","  print('ë‚˜ ğŸ˜€: {}'.format(sentence))\n","  print('ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: {}'.format(predicted_sentence))\n","\n","  return predicted_sentence"],"metadata":{"id":"V1g7SCRHs7PO","executionInfo":{"status":"ok","timestamp":1702922481921,"user_tz":-540,"elapsed":88,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def evaluate_2(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN_gr + tokenizer_gr.encode(sentence) + END_TOKEN_gr, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN_gr, 0)\n","\n","  # ë””ì½”ë”ì˜ ì˜ˆì¸¡ ì‹œì‘\n","  for i in range(20):\n","    predictions = loaded_model_2(inputs=[sentence, output], training=False)\n","\n","    # í˜„ì¬(ë§ˆì§€ë§‰) ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë°›ì•„ì˜¨ë‹¤.\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # ë§Œì•½ ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨\n","    if tf.equal(predicted_id, END_TOKEN_gr[0]):\n","      break\n","\n","    # ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ì¶œë ¥ì— ì—°ê²°í•œë‹¤.\n","    # ì´ëŠ” forë¬¸ì„ í†µí•´ì„œ ë””ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ì˜ˆì •ì´ë‹¤.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n","\n","\n","def predict_grammer(sentence):\n","  prediction = evaluate_2(sentence)\n","\n","  predicted_sentence = tokenizer_gr.decode(\n","      [i for i in prediction if i < tokenizer_gr.vocab_size])\n","\n","  print('ë‚˜ ğŸ˜€: {}'.format(sentence))\n","  print('ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): {}'.format(predicted_sentence))\n","\n","  return predicted_sentence"],"metadata":{"id":"i-WGZydPYfQk","executionInfo":{"status":"ok","timestamp":1702922481923,"user_tz":-540,"elapsed":85,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["#**1.**"],"metadata":{"id":"tr9aW2Sin_tf"}},{"cell_type":"code","source":["# ë¹„í˜•ì‹ì  ë¬¸ì¥ì„ ì¡´ëŒ“ë§/ê²©ì‹ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n","formal1 = predict_formal('ë‚˜ë„ ìŠ¤ì‹œ ë¨¹ê³  ì™€ì¨ ã…')"],"metadata":{"id":"NS-nOUDltgaS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c7c25e9-ed81-4e87-8262-9960c96b19c5","executionInfo":{"status":"ok","timestamp":1702922485333,"user_tz":-540,"elapsed":3492,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ë‚˜ë„ ìŠ¤ì‹œ ë¨¹ê³  ì™€ì¨ ã…\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ì €ë„ ìŠ¤ì‹œ ë¨¹ê³  ì™€ì¨ìš”.\n"]}]},{"cell_type":"code","source":["# ë§ì¶¤ë²• ì²˜ë¦¬\n","final_setence1 = predict_grammer(formal1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-MwYfF6nID5","outputId":"c13154c5-2022-4b78-9829-9faf1a24c9b8","executionInfo":{"status":"ok","timestamp":1702922488220,"user_tz":-540,"elapsed":2897,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì €ë„ ìŠ¤ì‹œ ë¨¹ê³  ì™€ì¨ìš”.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ì €ë„ ìŠ¤ì‹œ ë¨¹ê³  ì™”ì–´ìš”.\n"]}]},{"cell_type":"markdown","source":["#**2.**"],"metadata":{"id":"KJg0yofwoLS2"}},{"cell_type":"code","source":["formal2 = predict_formal('ì „í™”ì„ ëª»ë°›ì•„ ë¯¸ì•ˆí•´ìš”')"],"metadata":{"id":"IU_vzS20zism","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90f1441b-bf82-4caa-b382-c1691a470789","executionInfo":{"status":"ok","timestamp":1702922490362,"user_tz":-540,"elapsed":2190,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì „í™”ì„ ëª»ë°›ì•„ ë¯¸ì•ˆí•´ìš”\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ì—°ë½ì„ ëª»ë°›ì•„ ì£„ì†¡í•´ìš”.\n"]}]},{"cell_type":"code","source":["final_sentence2 = predict_grammer(formal2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCPYX0j7nfBF","outputId":"4cd8f24e-7fd0-4f5c-80ba-a1f348067a92","executionInfo":{"status":"ok","timestamp":1702922493431,"user_tz":-540,"elapsed":3075,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì—°ë½ì„ ëª»ë°›ì•„ ì£„ì†¡í•´ìš”.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ì—°ë½ì„ ëª» ë°›ì•„ ì£„ì†¡í•´ìš”.\n"]}]},{"cell_type":"markdown","source":["#**3.**"],"metadata":{"id":"DE0MKpKxoNTg"}},{"cell_type":"code","source":["formal3 = predict_formal('ë²Œì¨ ìˆ˜ì •ì´ ë‘ë²ˆì§¸ì•¼')"],"metadata":{"id":"xqqYTG6547EP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"acc88c03-4d30-439f-aac6-4bd5cf295ac1","executionInfo":{"status":"ok","timestamp":1702922495591,"user_tz":-540,"elapsed":2169,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ë²Œì¨ ìˆ˜ì •ì´ ë‘ë²ˆì§¸ì•¼\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ë²Œì¨ ìˆ˜ì •ì´ ë‘ë²ˆì§¸ì…ë‹ˆë‹¤.\n"]}]},{"cell_type":"code","source":["final_sentence2 = predict_grammer(formal3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHZmlr-PsAOH","outputId":"9aedb48b-d402-4998-b5ea-9877720794a7","executionInfo":{"status":"ok","timestamp":1702922498656,"user_tz":-540,"elapsed":3071,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ë²Œì¨ ìˆ˜ì •ì´ ë‘ë²ˆì§¸ì…ë‹ˆë‹¤.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ë²Œì¨ ìˆ˜ì •ì´ ë‘ ë²ˆì§¸ì…ë‹ˆë‹¤.\n"]}]},{"cell_type":"markdown","source":["----"],"metadata":{"id":"m3aLcMGytP5i"}},{"cell_type":"markdown","source":["#**4.**"],"metadata":{"id":"sPjZ61I0Aox8"}},{"cell_type":"code","source":["formal4 = predict_formal('ì˜¤ëˆŒ ì§€ê°ì´ë‹¤')"],"metadata":{"id":"Il4U1LVh-OVj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0f680c2-99f0-4668-bf1f-0d4d8c7f588e","executionInfo":{"status":"ok","timestamp":1702922502063,"user_tz":-540,"elapsed":3441,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì˜¤ëˆŒ ì§€ê°ì´ë‹¤\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ì˜¤ëˆŒ ì§€ê°ì´ì—ìš”.\n"]}]},{"cell_type":"code","source":["final_sentence4 = predict_grammer(formal4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozJocmFvpS3y","outputId":"c48dd7ca-0891-4e75-f797-af00633099fa","executionInfo":{"status":"ok","timestamp":1702922504800,"user_tz":-540,"elapsed":2756,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì˜¤ëˆŒ ì§€ê°ì´ì—ìš”.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ì˜¤ëŠ˜ ì§€ê°ì´ì—ìš”.\n"]}]},{"cell_type":"markdown","source":["#**5.**"],"metadata":{"id":"Hb8witddAzWX"}},{"cell_type":"code","source":["formal5 = predict_formal('ì˜¤ëŠ˜ìˆ˜ì—…ë‚´ìš© ëª¨ë¥´ê²Ÿì¨')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHGIA1qtAnYA","outputId":"9b1292a5-bbcf-418f-b4d0-dd333761403c","executionInfo":{"status":"ok","timestamp":1702922507398,"user_tz":-540,"elapsed":2606,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì˜¤ëŠ˜ìˆ˜ì—…ë‚´ìš© ëª¨ë¥´ê²Ÿì¨\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ì˜¤ëŠ˜ìˆ˜ì—…ë‚´ìš© ëª¨ë¥´ê²Ÿì¨ìš”.\n"]}]},{"cell_type":"code","source":["final_setence5 = predict_grammer(formal5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoHwUPGLDDU0","outputId":"267143a6-18dc-4ee0-bde6-ab50d7490a32","executionInfo":{"status":"ok","timestamp":1702922509668,"user_tz":-540,"elapsed":2277,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì˜¤ëŠ˜ìˆ˜ì—…ë‚´ìš© ëª¨ë¥´ê²Ÿì¨ìš”.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ì˜¤ëŠ˜ ìˆ˜ì—… ë‚´ìš© ëª¨ë¥´ê² ëŠ”ë°ìš”.\n"]}]},{"cell_type":"markdown","source":["#**7.**"],"metadata":{"id":"-7YVwhxCGKzq"}},{"cell_type":"code","source":["formal7 = predict_formal('ë‚˜ë„ ê°€ê³ ì‹œí”ˆë°')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDrzCucPGJXx","outputId":"14d5da9e-2897-4267-ea04-e3582bba1fe8","executionInfo":{"status":"ok","timestamp":1702922512621,"user_tz":-540,"elapsed":2971,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ë‚˜ë„ ê°€ê³ ì‹œí”ˆë°\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ì €ë„ ê°€ê³ ì‹œí”ˆë°ìš”.\n"]}]},{"cell_type":"code","source":["final_setence7 = predict_grammer(formal7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vi8cPOW2GLtj","outputId":"c99a4a00-f420-4404-d588-7fc122548360","executionInfo":{"status":"ok","timestamp":1702922514725,"user_tz":-540,"elapsed":2153,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ì €ë„ ê°€ê³ ì‹œí”ˆë°ìš”.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ì €ë„ ê°€ê³  ì‹¶ì€ë°ìš”.\n"]}]},{"cell_type":"markdown","source":["#**8.**"],"metadata":{"id":"-Qu3f2YbMPPQ"}},{"cell_type":"code","source":["formal9 = predict_formal('ê°ê¸°ê°€ìœ í–‰ì´ë‹ˆ ì¡°ì‹¬')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjaSpKKfMPhs","outputId":"1b42c660-792a-406c-fad3-b73b08ece634","executionInfo":{"status":"ok","timestamp":1702922517804,"user_tz":-540,"elapsed":3094,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ê°ê¸°ê°€ìœ í–‰ì´ë‹ˆ ì¡°ì‹¬\n","ì–´íˆ¬ ë³€ê²½ ë„ìš°ë¯¸ ì±—ë´‡ ğŸ¤–: ê°ê¸°ê°€ìœ í–‰ì´ë‹ˆ ì¡°ì‹¬í•˜ì„¸ìš”.\n"]}]},{"cell_type":"code","source":["final_setence9 = predict_grammer(formal9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3SwvCsz-MUdd","outputId":"15922f6a-0530-4b03-afa8-41775cd0d19e","executionInfo":{"status":"ok","timestamp":1702922521031,"user_tz":-540,"elapsed":3252,"user":{"displayName":"ì±„ì€","userId":"14441945920105052253"}}},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["ë‚˜ ğŸ˜€: ê°ê¸°ê°€ìœ í–‰ì´ë‹ˆ ì¡°ì‹¬í•˜ì„¸ìš”.\n","ì¤‘ìš”ë©”ì¼ ì‘ì„± ë„ìš°ë¯¸ ì±—ë´‡(ë§ì¶¤ë²• ì²˜ë¦¬ ì™„ë£Œ): ê°ê¸° ìœ í–‰ì´ë‹ˆ ì¡°ì‹¬í•˜ì„¸ìš”.\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}