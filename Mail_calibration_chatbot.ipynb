{"cells":[{"cell_type":"markdown","source":["#### 맞춤법의 토큰 처리만 이곳에서 하고 학습은 문법 전환 코드 파일에서 했습니다"],"metadata":{"id":"8UFMegY1vR9G"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"tA5mD5aGlBH3","executionInfo":{"status":"ok","timestamp":1702921949541,"user_tz":-540,"elapsed":25486,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["import pandas as pd\n","import urllib.request\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","from google.colab import drive\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","source":["# Google Drive 마운트\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMnjmxah_BdQ","outputId":"ec3a2b05-30c5-4128-ff33-d6f4fbdc15a0","executionInfo":{"status":"ok","timestamp":1702921953690,"user_tz":-540,"elapsed":4168,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YSdjdCs0lMMJ","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":2267,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["# 데이터가 저장된 디렉토리 설정\n","data_dir = '/content/gdrive/MyDrive/Colab Notebooks/'\n","\n","# 파일명 지정\n","file_name = 'combined.csv'\n","\n","file_name_grammer = 'spell_dataset7.csv'\n","\n","# 전체 파일 경로 생성\n","file_path = os.path.join(data_dir, file_name)\n","file_path_grammer = os.path.join(data_dir, file_name_grammer)\n","\n","# CSV 파일 읽기\n","df = pd.read_csv(file_path)\n","df_gr = pd.read_csv(file_path_grammer)"]},{"cell_type":"code","source":["print(file_path_grammer)"],"metadata":{"id":"sZ0U2OXCtx4z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b93ae552-6b59-4364-d3e1-1cbbdee953a1","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":4,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks/spell_dataset7.csv\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HxuiYiB_yHkj","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":2,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["from tensorflow.keras.optimizers.schedules import LearningRateSchedule"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fHMzeiROlPJp","executionInfo":{"status":"ok","timestamp":1702921955955,"user_tz":-540,"elapsed":2,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["# 최종 버전\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, position, d_model, name=\"positional_encoding\"):\n","        super(PositionalEncoding, self).__init__(name=name)\n","        self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","    def get_angles(self, position, i, d_model):\n","        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","        return position * angles\n","\n","    def positional_encoding(self, position, d_model):\n","        angle_rads = self.get_angles(\n","            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","            d_model=d_model\n","        )\n","\n","        sines = tf.math.sin(angle_rads[:, 0::2])\n","        cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","        pos_encoding = tf.concat([sines, cosines], axis=-1)\n","        pos_encoding = pos_encoding[tf.newaxis, ...]\n","\n","        return tf.cast(pos_encoding, tf.float32)\n","\n","    def call(self, inputs):\n","        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","    def get_config(self):\n","        return {}\n","\n","\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n","\n","  # Q와 K의 곱. 어텐션 스코어 행렬.\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 스케일링\n","  # dk의 루트값으로 나눠준다.\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n","  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n","  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    # d_model을 num_heads로 나눈 값.\n","    # 논문 기준 : 64\n","    self.depth = d_model // self.num_heads\n","\n","    # WQ, WK, WV에 해당하는 밀집층 정의\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    # WO에 해당하는 밀집층 정의\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  # num_heads 개수만큼 q, k, v를 split하는 함수\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n","    # q : (batch_size, query의 문장 길이, d_model)\n","    # k : (batch_size, key의 문장 길이, d_model)\n","    # v : (batch_size, value의 문장 길이, d_model)\n","    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 2. 헤드 나누기\n","    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n","    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n","    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 4. 헤드 연결(concatenate)하기\n","    # (batch_size, query의 문장 길이, d_model)\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 5. WO에 해당하는 밀집층 지나기\n","    # (batch_size, query의 문장 길이, d_model)\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs\n","\n","def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, key의 문장 길이)\n","  return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': padding_mask # 패딩 마스크 사용\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","def encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n","def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n","  return tf.maximum(look_ahead_mask, padding_mask)\n","\n","def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': look_ahead_mask # 룩어헤드 마스크\n","      })\n","\n","  # 잔차 연결과 층 정규화\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n","          'mask': padding_mask # 패딩 마스크\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def decoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 디코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def transformer(vocab_size, num_layers, dff,\n","                d_model, num_heads, dropout,\n","                name=\"transformer\"):\n","\n","  # 인코더의 입력\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 디코더의 입력\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  # 인코더의 패딩 마스크\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","\n","  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask, output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","\n","  # 디코더의 패딩 마스크(두번째 서브층)\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n","  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n","\n","  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n","  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  # 다음 단어 예측을 위한 출력층\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n","\n","def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, dtype=tf.float32)\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","    def get_config(self):\n","        return {\n","            'd_model': self.d_model.numpy(),\n","            'warmup_steps': self.warmup_steps,\n","        }\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"YTKKZsGLosDm","executionInfo":{"status":"ok","timestamp":1702921957015,"user_tz":-540,"elapsed":1062,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["def preprocess_sentence(sentence):\n","  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","  sentence = sentence.strip()\n","  return sentence"]},{"cell_type":"code","source":["# NULL 값이 있는 행을 제거\n","df = df.dropna()\n","df_gr = df_gr.dropna()\n","\n","# 제거된 후의 데이터 확인\n","print(df.isnull().sum())\n","print(df_gr.isnull().sum())"],"metadata":{"id":"7R3iIZ_nbxu1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7403f5c7-cc0a-4a84-9295-681dff542603","executionInfo":{"status":"ok","timestamp":1702921957015,"user_tz":-540,"elapsed":3,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Q    0\n","A    0\n","dtype: int64\n","Q    0\n","A    0\n","dtype: int64\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KNMY75I6lf6S","executionInfo":{"status":"ok","timestamp":1702921967062,"user_tz":-540,"elapsed":10049,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["questions = []\n","for sentence in df['Q']:\n","    # 구두점에 대해서 띄어쓰기\n","    # ex) 12시 땡! -> 12시 땡 !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    questions.append(sentence)\n","\n","answers = []\n","for sentence in df['A']:\n","    # 구두점에 대해서 띄어쓰기\n","    # ex) 12시 땡! -> 12시 땡 !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    answers.append(sentence)\n","\n","questions_gr = []\n","for sentence in df_gr['Q']:\n","    # 구두점에 대해서 띄어쓰기\n","    # ex) 12시 땡! -> 12시 땡 !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    questions_gr.append(sentence)\n","\n","answers_gr = []\n","for sentence in df_gr['A']:\n","    # 구두점에 대해서 띄어쓰기\n","    # ex) 12시 땡! -> 12시 땡 !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    answers_gr.append(sentence)\n","\n","\n","# Assuming df is the DataFrame containing your data\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Drop rows with NaN values\n","train_df = train_df.dropna()\n","test_df = test_df.dropna()\n","\n","\n","train_questions = train_df['Q']\n","train_answers = train_df['A']\n","\n","test_questions = test_df['Q']\n","test_answers = test_df['A']\n","\n","train_df_gr, test_df_gr = train_test_split(df_gr, test_size=0.2, random_state=42)\n","\n","train_df_gr = train_df_gr.dropna()\n","test_df_gr = test_df_gr.dropna()\n","\n","train_questions_gr = train_df_gr['Q']\n","train_answers_gr = train_df_gr['A']\n","\n","test_questions_gr = test_df_gr['Q']\n","test_answers_gr = test_df_gr['A']"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PdBQBr2Clh7i","executionInfo":{"status":"ok","timestamp":1702922186611,"user_tz":-540,"elapsed":217398,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["# 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n","tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**13)\n","\n","# 시작 토큰과 종료 토큰에 대한 정수 부여.\n","\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","VOCAB_SIZE = tokenizer.vocab_size + 2\n","MAX_LENGTH = 10"]},{"cell_type":"code","source":["# 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n","tokenizer_gr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions_gr + answers_gr, target_vocab_size=2**13)\n","\n","# 시작 토큰과 종료 토큰에 대한 정수 부여.\n","\n","START_TOKEN_gr, END_TOKEN_gr = [tokenizer_gr.vocab_size], [tokenizer_gr.vocab_size + 1]\n","VOCAB_SIZE_gr = tokenizer_gr.vocab_size + 2"],"metadata":{"id":"ZUJ1DUttjAG3","executionInfo":{"status":"ok","timestamp":1702922393606,"user_tz":-540,"elapsed":207093,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","\n","    tokenized_inputs.append(sentence1)\n","    tokenized_outputs.append(sentence2)\n","\n","  # 패딩\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs\n","\n","questions, answers = tokenize_and_filter(questions, answers)\n","\n","train_questions, train_answers = tokenize_and_filter(train_questions, train_answers)\n","test_questions, test_answers = tokenize_and_filter(test_questions, test_answers)\n","\n","def tokenize_and_filter_gr(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n","    sentence1 = START_TOKEN_gr + tokenizer.encode(sentence1) + END_TOKEN_gr\n","    sentence2 = START_TOKEN_gr + tokenizer.encode(sentence2) + END_TOKEN_gr\n","\n","    tokenized_inputs.append(sentence1)\n","    tokenized_outputs.append(sentence2)\n","\n","  # 패딩\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=20, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=20, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs\n","\n","questions_gr, answers_gr = tokenize_and_filter_gr(questions_gr, answers_gr)\n","\n","train_questions_gr, train_answers_gr = tokenize_and_filter_gr(train_questions_gr, train_answers_gr)\n","test_questions_gr, test_answers_gr = tokenize_and_filter_gr(test_questions_gr, test_answers_gr)"],"metadata":{"id":"c8X0F6vHZzYk","executionInfo":{"status":"ok","timestamp":1702922430370,"user_tz":-540,"elapsed":36835,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"jtMhk_LelxG6","executionInfo":{"status":"ok","timestamp":1702922430398,"user_tz":-540,"elapsed":943,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n","# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","# 훈련 데이터셋 생성\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': train_questions,\n","        'dec_inputs': train_answers[:, :-1]\n","    },\n","    {\n","        'outputs': train_answers[:, 1:]\n","    },\n","))\n","\n","# 셔플, 배치 및 교사 강요 설정\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# 테스트 데이터셋 생성\n","test_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': test_questions,\n","        'dec_inputs': test_answers[:, :-1]\n","    },\n","    {\n","        'outputs': test_answers[:, 1:]\n","    },\n","))\n","\n","# 배치 설정\n","test_dataset = test_dataset.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"0bLKqU_kmBeq","executionInfo":{"status":"ok","timestamp":1702920411655,"user_tz":-540,"elapsed":7899,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["# Hyper-parameters\n","VOCAB_SIZE = 10000\n","NUM_LAYERS = 4\n","D_MODEL = 256\n","NUM_HEADS = 8\n","DFF = 512\n","DROPOUT = 0.1\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    dff=DFF,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"oN-hVP2ZmGdY","executionInfo":{"status":"ok","timestamp":1702920411656,"user_tz":-540,"elapsed":49,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["MAX_LENGTH = 10\n","\n","\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"]},{"cell_type":"code","source":["EPOCHS = 10\n","\n","# 모델 학습\n","model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset)\n","\n","\n","# 정확도 시각화\n","import matplotlib.pyplot as plt\n","\n","# Assuming that 'accuracy' is a metric in your model\n","plt.plot(model.history.history['accuracy'], label='Training Accuracy')\n","plt.plot(model.history.history['val_accuracy'], label='Testing Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"id":"Zuh6M6UbhnQI","colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"status":"error","timestamp":1702921685100,"user_tz":-540,"elapsed":1273490,"user":{"displayName":"채은","userId":"14441945920105052253"}},"outputId":"deb3d786-94c9-4b34-e317-86645b825a39"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1240/1724 [====================>.........] - ETA: 8:04 - loss: 5.3297 - accuracy: 0.2695"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-365273ba3799>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyGfCKSD2DOA","executionInfo":{"status":"aborted","timestamp":1702921685101,"user_tz":-540,"elapsed":6,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["model.save('/content/gdrive/My Drive/Colab Notebooks/model_formal', save_format='tf')  # Save as TensorFlow SavedModel\n","# or"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3cso6Bp13J6l","executionInfo":{"status":"ok","timestamp":1702922456609,"user_tz":-540,"elapsed":27147,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"outputs":[],"source":["# informal -> formal 변환 학습 모델\n","with tf.keras.utils.custom_object_scope({'CustomSchedule': CustomSchedule, 'loss_function': loss_function}):\n","    loaded_model_1 = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_formal', custom_objects={'MultiHeadAttention': MultiHeadAttention})"]},{"cell_type":"code","source":["# 맞춤법 처리 학습 모델\n","with tf.keras.utils.custom_object_scope({'CustomSchedule': CustomSchedule, 'loss_function': loss_function}):\n","    loaded_model_2 = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/model_grammer', custom_objects={'MultiHeadAttention': MultiHeadAttention})"],"metadata":{"id":"wT0XDwaF8FSF","executionInfo":{"status":"ok","timestamp":1702922481917,"user_tz":-540,"elapsed":25375,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(test_dataset)\n","print(f'Test Accuracy: {test_accuracy}')"],"metadata":{"id":"gTrIPsx1rN7u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7621460d-2fc7-42a7-f28b-913e39ac7b18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["432/432 [==============================] - 151s 342ms/step - loss: 4.0398 - accuracy: 0.3733\n"]}]},{"cell_type":"code","source":["def evaluate_1(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN, 0)\n","\n","  # 디코더의 예측 시작\n","  for i in range(MAX_LENGTH):\n","    predictions = loaded_model_1(inputs=[sentence, output], training=False)\n","\n","    # 현재(마지막) 시점의 예측 단어를 받아온다.\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    # 마지막 시점의 예측 단어를 출력에 연결한다.\n","    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n","\n","\n","def predict_formal(sentence):\n","  prediction = evaluate_1(sentence)\n","\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i < tokenizer.vocab_size])\n","\n","  print('나 😀: {}'.format(sentence))\n","  print('어투 변경 도우미 챗봇 🤖: {}'.format(predicted_sentence))\n","\n","  return predicted_sentence"],"metadata":{"id":"V1g7SCRHs7PO","executionInfo":{"status":"ok","timestamp":1702922481921,"user_tz":-540,"elapsed":88,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def evaluate_2(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN_gr + tokenizer_gr.encode(sentence) + END_TOKEN_gr, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN_gr, 0)\n","\n","  # 디코더의 예측 시작\n","  for i in range(20):\n","    predictions = loaded_model_2(inputs=[sentence, output], training=False)\n","\n","    # 현재(마지막) 시점의 예측 단어를 받아온다.\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","    if tf.equal(predicted_id, END_TOKEN_gr[0]):\n","      break\n","\n","    # 마지막 시점의 예측 단어를 출력에 연결한다.\n","    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n","\n","\n","def predict_grammer(sentence):\n","  prediction = evaluate_2(sentence)\n","\n","  predicted_sentence = tokenizer_gr.decode(\n","      [i for i in prediction if i < tokenizer_gr.vocab_size])\n","\n","  print('나 😀: {}'.format(sentence))\n","  print('중요메일 작성 도우미 챗봇(맞춤법 처리 완료): {}'.format(predicted_sentence))\n","\n","  return predicted_sentence"],"metadata":{"id":"i-WGZydPYfQk","executionInfo":{"status":"ok","timestamp":1702922481923,"user_tz":-540,"elapsed":85,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["#**1.**"],"metadata":{"id":"tr9aW2Sin_tf"}},{"cell_type":"code","source":["# 비형식적 문장을 존댓말/격식있는 문장으로 변환\n","formal1 = predict_formal('나도 스시 먹고 와써 ㅎ')"],"metadata":{"id":"NS-nOUDltgaS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c7c25e9-ed81-4e87-8262-9960c96b19c5","executionInfo":{"status":"ok","timestamp":1702922485333,"user_tz":-540,"elapsed":3492,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 나도 스시 먹고 와써 ㅎ\n","어투 변경 도우미 챗봇 🤖: 저도 스시 먹고 와써요.\n"]}]},{"cell_type":"code","source":["# 맞춤법 처리\n","final_setence1 = predict_grammer(formal1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-MwYfF6nID5","outputId":"c13154c5-2022-4b78-9829-9faf1a24c9b8","executionInfo":{"status":"ok","timestamp":1702922488220,"user_tz":-540,"elapsed":2897,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 저도 스시 먹고 와써요.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 저도 스시 먹고 왔어요.\n"]}]},{"cell_type":"markdown","source":["#**2.**"],"metadata":{"id":"KJg0yofwoLS2"}},{"cell_type":"code","source":["formal2 = predict_formal('전화을 못받아 미안해요')"],"metadata":{"id":"IU_vzS20zism","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90f1441b-bf82-4caa-b382-c1691a470789","executionInfo":{"status":"ok","timestamp":1702922490362,"user_tz":-540,"elapsed":2190,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 전화을 못받아 미안해요\n","어투 변경 도우미 챗봇 🤖: 연락을 못받아 죄송해요.\n"]}]},{"cell_type":"code","source":["final_sentence2 = predict_grammer(formal2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCPYX0j7nfBF","outputId":"4cd8f24e-7fd0-4f5c-80ba-a1f348067a92","executionInfo":{"status":"ok","timestamp":1702922493431,"user_tz":-540,"elapsed":3075,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 연락을 못받아 죄송해요.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 연락을 못 받아 죄송해요.\n"]}]},{"cell_type":"markdown","source":["#**3.**"],"metadata":{"id":"DE0MKpKxoNTg"}},{"cell_type":"code","source":["formal3 = predict_formal('벌써 수정이 두번째야')"],"metadata":{"id":"xqqYTG6547EP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"acc88c03-4d30-439f-aac6-4bd5cf295ac1","executionInfo":{"status":"ok","timestamp":1702922495591,"user_tz":-540,"elapsed":2169,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 벌써 수정이 두번째야\n","어투 변경 도우미 챗봇 🤖: 벌써 수정이 두번째입니다.\n"]}]},{"cell_type":"code","source":["final_sentence2 = predict_grammer(formal3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHZmlr-PsAOH","outputId":"9aedb48b-d402-4998-b5ea-9877720794a7","executionInfo":{"status":"ok","timestamp":1702922498656,"user_tz":-540,"elapsed":3071,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 벌써 수정이 두번째입니다.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 벌써 수정이 두 번째입니다.\n"]}]},{"cell_type":"markdown","source":["----"],"metadata":{"id":"m3aLcMGytP5i"}},{"cell_type":"markdown","source":["#**4.**"],"metadata":{"id":"sPjZ61I0Aox8"}},{"cell_type":"code","source":["formal4 = predict_formal('오눌 지각이다')"],"metadata":{"id":"Il4U1LVh-OVj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0f680c2-99f0-4668-bf1f-0d4d8c7f588e","executionInfo":{"status":"ok","timestamp":1702922502063,"user_tz":-540,"elapsed":3441,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 오눌 지각이다\n","어투 변경 도우미 챗봇 🤖: 오눌 지각이에요.\n"]}]},{"cell_type":"code","source":["final_sentence4 = predict_grammer(formal4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozJocmFvpS3y","outputId":"c48dd7ca-0891-4e75-f797-af00633099fa","executionInfo":{"status":"ok","timestamp":1702922504800,"user_tz":-540,"elapsed":2756,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 오눌 지각이에요.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 오늘 지각이에요.\n"]}]},{"cell_type":"markdown","source":["#**5.**"],"metadata":{"id":"Hb8witddAzWX"}},{"cell_type":"code","source":["formal5 = predict_formal('오늘수업내용 모르겟써')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHGIA1qtAnYA","outputId":"9b1292a5-bbcf-418f-b4d0-dd333761403c","executionInfo":{"status":"ok","timestamp":1702922507398,"user_tz":-540,"elapsed":2606,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 오늘수업내용 모르겟써\n","어투 변경 도우미 챗봇 🤖: 오늘수업내용 모르겟써요.\n"]}]},{"cell_type":"code","source":["final_setence5 = predict_grammer(formal5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoHwUPGLDDU0","outputId":"267143a6-18dc-4ee0-bde6-ab50d7490a32","executionInfo":{"status":"ok","timestamp":1702922509668,"user_tz":-540,"elapsed":2277,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 오늘수업내용 모르겟써요.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 오늘 수업 내용 모르겠는데요.\n"]}]},{"cell_type":"markdown","source":["#**7.**"],"metadata":{"id":"-7YVwhxCGKzq"}},{"cell_type":"code","source":["formal7 = predict_formal('나도 가고시픈데')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDrzCucPGJXx","outputId":"14d5da9e-2897-4267-ea04-e3582bba1fe8","executionInfo":{"status":"ok","timestamp":1702922512621,"user_tz":-540,"elapsed":2971,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 나도 가고시픈데\n","어투 변경 도우미 챗봇 🤖: 저도 가고시픈데요.\n"]}]},{"cell_type":"code","source":["final_setence7 = predict_grammer(formal7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vi8cPOW2GLtj","outputId":"c99a4a00-f420-4404-d588-7fc122548360","executionInfo":{"status":"ok","timestamp":1702922514725,"user_tz":-540,"elapsed":2153,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 저도 가고시픈데요.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 저도 가고 싶은데요.\n"]}]},{"cell_type":"markdown","source":["#**8.**"],"metadata":{"id":"-Qu3f2YbMPPQ"}},{"cell_type":"code","source":["formal9 = predict_formal('감기가유행이니 조심')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjaSpKKfMPhs","outputId":"1b42c660-792a-406c-fad3-b73b08ece634","executionInfo":{"status":"ok","timestamp":1702922517804,"user_tz":-540,"elapsed":3094,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 감기가유행이니 조심\n","어투 변경 도우미 챗봇 🤖: 감기가유행이니 조심하세요.\n"]}]},{"cell_type":"code","source":["final_setence9 = predict_grammer(formal9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3SwvCsz-MUdd","outputId":"15922f6a-0530-4b03-afa8-41775cd0d19e","executionInfo":{"status":"ok","timestamp":1702922521031,"user_tz":-540,"elapsed":3252,"user":{"displayName":"채은","userId":"14441945920105052253"}}},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["나 😀: 감기가유행이니 조심하세요.\n","중요메일 작성 도우미 챗봇(맞춤법 처리 완료): 감기 유행이니 조심하세요.\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}